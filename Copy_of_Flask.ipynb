{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Indirapriyadarshini/Document-Summerization/blob/main/Copy_of_Flask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kzs9wthLFbJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/template')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "T6fNGHM0h1oQ",
        "outputId": "939b8c60-e9d8-4970-9896-d76a85a1ab97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nezzKuLNzE9"
      },
      "outputs": [],
      "source": [
        "pip install flask-ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FG3XSmFAZFRB",
        "outputId": "da59297b-000d-45ec-d4c5-09e8ea1f6597"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sumy in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
            "Requirement already satisfied: docopt<0.7,>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from sumy) (0.6.2)\n",
            "Requirement already satisfied: breadability>=0.1.20 in /usr/local/lib/python3.10/dist-packages (from sumy) (0.1.20)\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from sumy) (2.27.1)\n",
            "Requirement already satisfied: pycountry>=18.2.23 in /usr/local/lib/python3.10/dist-packages (from sumy) (22.3.5)\n",
            "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from sumy) (3.8.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (4.0.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (4.9.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (4.65.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pycountry>=18.2.23->sumy) (67.7.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install sumy\n",
        "import sumy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9B_HdCJnIImm"
      },
      "outputs": [],
      "source": [
        "from flask import Flask,render_template,url_for,request\n",
        "import time\n",
        "import spacy\n",
        "import nltk\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "from sumy.summarizers.luhn import LuhnSummarizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import urlopen,Request"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcoCW-5kIIpg"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ozjxRwnIIs7"
      },
      "outputs": [],
      "source": [
        "app = Flask(__name__)\n",
        "\n",
        "def lex_summary(docx):\n",
        "\tparser = PlaintextParser.from_string(docx,Tokenizer(\"english\"))\n",
        "\tlex_summarizer = LexRankSummarizer()\n",
        "\tsummary = lex_summarizer(parser.document,3)\n",
        "\tsummary_list = [str(sentence) for sentence in summary]\n",
        "\tresult = ' '.join(summary_list)\n",
        "\treturn result\n",
        "\n",
        "def luhn_summary(docx):\n",
        "\tparser = PlaintextParser.from_string(docx,Tokenizer(\"english\"))\n",
        "\tsummarizer_luhn = LuhnSummarizer()\n",
        "\tsummary_1 =summarizer_luhn(parser.document,3)\n",
        "\tsummary_list = [str(sentence) for sentence in summary_1]\n",
        "\tresult = ' '.join(summary_list)\n",
        "\treturn result\n",
        "\n",
        "\n",
        "def isa_summary(docx):\n",
        "\tparser = PlaintextParser.from_string(docx,Tokenizer(\"english\"))\n",
        "\tsummarizer_lsa = LsaSummarizer()\n",
        "\tsummary_2 =summarizer_lsa(parser.document,3)\n",
        "\tsummary_list = [str(sentence) for sentence in summary_2]\n",
        "\tresult = ' '.join(summary_list)\n",
        "\treturn result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1MI4SwvIIvV"
      },
      "outputs": [],
      "source": [
        "# Reading Time\n",
        "def readingTime(mytext):\n",
        "\ttotal_words = len([ token.text for token in nlp(mytext)])\n",
        "\testimatedTime = total_words/200.0\n",
        "\treturn estimatedTime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhlR3sK9IIy3"
      },
      "outputs": [],
      "source": [
        "@app.route('/')\n",
        "def index():\n",
        "\treturn render_template('index.html')\n",
        "\n",
        "@app.route('/process',methods=['GET','POST'])\n",
        "def process():\n",
        "    start = time.time()\n",
        "    if request.method == 'POST':\n",
        "        input_text = request.form['input_text']\n",
        "        model_choice = request.form['model_choice']\n",
        "        final_reading_time = readingTime(input_text)\n",
        "        if model_choice == 'default':\n",
        "            final_summary = lex_summary(input_text)\n",
        "        elif model_choice == 'lex_summarizer':\n",
        "            final_summary = lex_summary(input_text)\n",
        "        elif model_choice == 'luhn_summarizer':\n",
        "            final_summary= luhn_summary(input_text)\n",
        "        elif model_choice == 'isa_summarizer':\n",
        "            final_summary= isa_summary(input_text)\n",
        "    summary_reading_time = readingTime(final_summary)\n",
        "    end = time.time()\n",
        "    final_time = end-start\n",
        "    return render_template('summary.html',ctext=input_text,final_reading_time=final_reading_time,summary_reading_time=summary_reading_time,final_summary=final_summary,model_selected=model_choice)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBXgK7aXIx8-"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "# from urllib.request import urlopen\n",
        "from urllib.request import urlopen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "petIRsyRIyAP"
      },
      "outputs": [],
      "source": [
        "def get_text(url):\n",
        "    reqt = Request(url,headers={'User-Agent' : \"Magic Browser\"})\n",
        "    page = urlopen(reqt)\n",
        "    soup = BeautifulSoup(page)\n",
        "    fetched_text = ' '.join(map(lambda p:p.text,soup.find_all('p')))\n",
        "    return fetched_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LE8mSs-I5d3"
      },
      "outputs": [],
      "source": [
        "@app.route('/process_url',methods=['GET','POST'])\n",
        "def process_url():\n",
        "\tstart = time.time()\n",
        "\tif request.method == 'POST':\n",
        "\t\tinput_url = request.form['input_url']\n",
        "\t\traw_text = get_text(input_url)\n",
        "\t\tfinal_reading_time = readingTime(raw_text)\n",
        "\t\tfinal_summary = lex_summary(raw_text)\n",
        "\t\tsummary_reading_time = readingTime(final_summary)\n",
        "\t\tend = time.time()\n",
        "\t\tfinal_time = end-start\n",
        "\treturn render_template('summary.html',ctext=raw_text,\n",
        "                        final_summary=final_summary,\n",
        "                        final_time=final_time,\n",
        "                        final_reading_time=final_reading_time,\n",
        "                        summary_reading_time=summary_reading_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_lFf_txOolH",
        "outputId": "f339ffeb-ee64-46db-ec7b-f4c4c52b5034"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug: * Restarting with stat\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "\tapp.run(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAoGeCGOOon0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrvEcIPrU8k4"
      },
      "source": [
        "**Mounting Google Colab Terminal**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngJZdeD2Ooqq"
      },
      "outputs": [],
      "source": [
        "from IPython.display import JSON\n",
        "from google.colab import output\n",
        "from subprocess import getoutput\n",
        "import os\n",
        "def shell(command):\n",
        "  if command.startswith('cd'):\n",
        "    path = command.strip().split(maxsplit=1)[1]\n",
        "    os.chdir(path)\n",
        "    return JSON([''])\n",
        "  return JSON([getoutput(command)])\n",
        "output.register_callback('shell', shell)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "scY1CQ1iOosv",
        "outputId": "41cc6ed9-8888-4d12-cc25-1bf7c313a379"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div id=term_demo></div>\n",
              "<script src=\"https://code.jquery.com/jquery-latest.js\"></script>\n",
              "<script src=\"https://cdn.jsdelivr.net/npm/jquery.terminal/js/jquery.terminal.min.js\"></script>\n",
              "<link href=\"https://cdn.jsdelivr.net/npm/jquery.terminal/css/jquery.terminal.min.css\" rel=\"stylesheet\"/>\n",
              "<script>\n",
              "  $('#term_demo').terminal(async function(command) {\n",
              "      if (command !== '') {\n",
              "          try {\n",
              "              let res = await google.colab.kernel.invokeFunction('shell', [command])\n",
              "              let out = res.data['application/json'][0]\n",
              "              this.echo(new String(out))\n",
              "          } catch(e) {\n",
              "              this.error(new String(e));\n",
              "          }\n",
              "      } else {\n",
              "          this.echo('');\n",
              "      }\n",
              "  }, {\n",
              "      greetings: 'Welcome to Colab Shell',\n",
              "      name: 'colab_demo',\n",
              "      height: 250,\n",
              "      prompt: 'colab > '\n",
              "  });\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title Colab Shell\n",
        "%%html\n",
        "<div id=term_demo></div>\n",
        "<script src=\"https://code.jquery.com/jquery-latest.js\"></script>\n",
        "<script src=\"https://cdn.jsdelivr.net/npm/jquery.terminal/js/jquery.terminal.min.js\"></script>\n",
        "<link href=\"https://cdn.jsdelivr.net/npm/jquery.terminal/css/jquery.terminal.min.css\" rel=\"stylesheet\"/>\n",
        "<script>\n",
        "  $('#term_demo').terminal(async function(command) {\n",
        "      if (command !== '') {\n",
        "          try {\n",
        "              let res = await google.colab.kernel.invokeFunction('shell', [command])\n",
        "              let out = res.data['application/json'][0]\n",
        "              this.echo(new String(out))\n",
        "          } catch(e) {\n",
        "              this.error(new String(e));\n",
        "          }\n",
        "      } else {\n",
        "          this.echo('');\n",
        "      }\n",
        "  }, {\n",
        "      greetings: 'Welcome to Colab Shell',\n",
        "      name: 'colab_demo',\n",
        "      height: 250,\n",
        "      prompt: 'colab > '\n",
        "  });"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fh8KD0ylb3Wu",
        "outputId": "39f3a6ee-88f1-438f-e2e4-30aeaff4ac37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scrubadub\n",
            "  Using cached scrubadub-2.0.0-py3-none-any.whl (65 kB)\n",
            "Collecting textblob==0.15.3 (from scrubadub)\n",
            "  Using cached textblob-0.15.3-py2.py3-none-any.whl (636 kB)\n",
            "Collecting phonenumbers (from scrubadub)\n",
            "  Using cached phonenumbers-8.13.13-py2.py3-none-any.whl (2.6 MB)\n",
            "Collecting python-stdnum (from scrubadub)\n",
            "  Using cached python_stdnum-1.18-py2.py3-none-any.whl (1.0 MB)\n",
            "Collecting dateparser (from scrubadub)\n",
            "  Using cached dateparser-1.1.8-py2.py3-none-any.whl (293 kB)\n",
            "Requirement already satisfied: catalogue in /usr/local/lib/python3.10/dist-packages (from scrubadub) (2.0.8)\n",
            "Collecting sklearn (from scrubadub)\n",
            "  Using cached sklearn-0.0.post5.tar.gz (3.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from scrubadub) (4.5.0)\n",
            "Collecting faker (from scrubadub)\n",
            "  Downloading Faker-18.10.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob==0.15.3->scrubadub) (3.8.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from dateparser->scrubadub) (2.8.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from dateparser->scrubadub) (2022.7.1)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in /usr/local/lib/python3.10/dist-packages (from dateparser->scrubadub) (2022.10.31)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from dateparser->scrubadub) (4.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob==0.15.3->scrubadub) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob==0.15.3->scrubadub) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob==0.15.3->scrubadub) (4.65.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->dateparser->scrubadub) (1.16.0)\n",
            "Requirement already satisfied: pytz-deprecation-shim in /usr/local/lib/python3.10/dist-packages (from tzlocal->dateparser->scrubadub) (0.1.0.post0)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.10/dist-packages (from pytz-deprecation-shim->tzlocal->dateparser->scrubadub) (2023.3)\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post5-py3-none-any.whl size=2950 sha256=fd25c6d526690bedb66f510f08dac3acdb98be019b7457564f9c7aef01ca476c\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/1f/8d/4f812c590e074c1e928f5cec67bf5053b71f38e2648739403a\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn, python-stdnum, phonenumbers, textblob, faker, dateparser, scrubadub\n",
            "  Attempting uninstall: textblob\n",
            "    Found existing installation: textblob 0.17.1\n",
            "    Uninstalling textblob-0.17.1:\n",
            "      Successfully uninstalled textblob-0.17.1\n",
            "Successfully installed dateparser-1.1.8 faker-18.10.1 phonenumbers-8.13.13 python-stdnum-1.18 scrubadub-2.0.0 sklearn-0.0.post5 textblob-0.15.3\n"
          ]
        }
      ],
      "source": [
        "!pip install scrubadub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u89iEZYjA5Zz",
        "outputId": "60fd2ec1-d97e-461d-9f6b-59cdccb42447"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scrubadub_spacy\n",
            "  Using cached scrubadub_spacy-2.0.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: scrubadub>=2.0.0rc0 in /usr/local/lib/python3.10/dist-packages (from scrubadub_spacy) (2.0.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from scrubadub_spacy) (0.40.0)\n",
            "Requirement already satisfied: spacy[transformers]>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from scrubadub_spacy) (3.5.2)\n",
            "Requirement already satisfied: textblob==0.15.3 in /usr/local/lib/python3.10/dist-packages (from scrubadub>=2.0.0rc0->scrubadub_spacy) (0.15.3)\n",
            "Requirement already satisfied: phonenumbers in /usr/local/lib/python3.10/dist-packages (from scrubadub>=2.0.0rc0->scrubadub_spacy) (8.13.13)\n",
            "Requirement already satisfied: python-stdnum in /usr/local/lib/python3.10/dist-packages (from scrubadub>=2.0.0rc0->scrubadub_spacy) (1.18)\n",
            "Requirement already satisfied: dateparser in /usr/local/lib/python3.10/dist-packages (from scrubadub>=2.0.0rc0->scrubadub_spacy) (1.1.8)\n",
            "Requirement already satisfied: catalogue in /usr/local/lib/python3.10/dist-packages (from scrubadub>=2.0.0rc0->scrubadub_spacy) (2.0.8)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.10/dist-packages (from scrubadub>=2.0.0rc0->scrubadub_spacy) (0.0.post5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from scrubadub>=2.0.0rc0->scrubadub_spacy) (4.5.0)\n",
            "Requirement already satisfied: faker in /usr/local/lib/python3.10/dist-packages (from scrubadub>=2.0.0rc0->scrubadub_spacy) (18.10.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob==0.15.3->scrubadub>=2.0.0rc0->scrubadub_spacy) (3.8.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy[transformers]>=3.0.0->scrubadub_spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy[transformers]>=3.0.0->scrubadub_spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy[transformers]>=3.0.0->scrubadub_spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy[transformers]>=3.0.0->scrubadub_spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy[transformers]>=3.0.0->scrubadub_spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy[transformers]>=3.0.0->scrubadub_spacy) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy[transformers]>=3.0.0->scrubadub_spacy) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy[transformers]>=3.0.0->scrubadub_spacy) (2.4.6)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy[transformers]>=3.0.0->scrubadub_spacy) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy[transformers]>=3.0.0->scrubadub_spacy) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy[transformers]>=3.0.0->scrubadub_spacy) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy[transformers]>=3.0.0->scrubadub_spacy) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy[transformers]>=3.0.0->scrubadub_spacy) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy[transformers]>=3.0.0->scrubadub_spacy) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy[transformers]>=3.0.0->scrubadub_spacy) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy[transformers]>=3.0.0->scrubadub_spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy[transformers]>=3.0.0->scrubadub_spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy[transformers]>=3.0.0->scrubadub_spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy[transformers]>=3.0.0->scrubadub_spacy) (3.3.0)\n",
            "Requirement already satisfied: spacy-transformers<1.3.0,>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from spacy[transformers]>=3.0.0->scrubadub_spacy) (1.2.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]>=3.0.0->scrubadub_spacy) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]>=3.0.0->scrubadub_spacy) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]>=3.0.0->scrubadub_spacy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]>=3.0.0->scrubadub_spacy) (3.4)\n",
            "Requirement already satisfied: transformers<4.30.0,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers<1.3.0,>=1.1.2->spacy[transformers]>=3.0.0->scrubadub_spacy) (4.29.2)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers<1.3.0,>=1.1.2->spacy[transformers]>=3.0.0->scrubadub_spacy) (2.0.1+cu118)\n",
            "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers<1.3.0,>=1.1.2->spacy[transformers]>=3.0.0->scrubadub_spacy) (0.9.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy[transformers]>=3.0.0->scrubadub_spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy[transformers]>=3.0.0->scrubadub_spacy) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy[transformers]>=3.0.0->scrubadub_spacy) (8.1.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from dateparser->scrubadub>=2.0.0rc0->scrubadub_spacy) (2.8.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from dateparser->scrubadub>=2.0.0rc0->scrubadub_spacy) (2022.7.1)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in /usr/local/lib/python3.10/dist-packages (from dateparser->scrubadub>=2.0.0rc0->scrubadub_spacy) (2022.10.31)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from dateparser->scrubadub>=2.0.0rc0->scrubadub_spacy) (4.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy[transformers]>=3.0.0->scrubadub_spacy) (2.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob==0.15.3->scrubadub>=2.0.0rc0->scrubadub_spacy) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->dateparser->scrubadub>=2.0.0rc0->scrubadub_spacy) (1.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.1.2->spacy[transformers]>=3.0.0->scrubadub_spacy) (3.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.1.2->spacy[transformers]>=3.0.0->scrubadub_spacy) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.1.2->spacy[transformers]>=3.0.0->scrubadub_spacy) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.1.2->spacy[transformers]>=3.0.0->scrubadub_spacy) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->spacy-transformers<1.3.0,>=1.1.2->spacy[transformers]>=3.0.0->scrubadub_spacy) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->spacy-transformers<1.3.0,>=1.1.2->spacy[transformers]>=3.0.0->scrubadub_spacy) (16.0.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.30.0,>=3.4.0->spacy-transformers<1.3.0,>=1.1.2->spacy[transformers]>=3.0.0->scrubadub_spacy) (0.15.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.30.0,>=3.4.0->spacy-transformers<1.3.0,>=1.1.2->spacy[transformers]>=3.0.0->scrubadub_spacy) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.30.0,>=3.4.0->spacy-transformers<1.3.0,>=1.1.2->spacy[transformers]>=3.0.0->scrubadub_spacy) (0.13.3)\n",
            "Requirement already satisfied: pytz-deprecation-shim in /usr/local/lib/python3.10/dist-packages (from tzlocal->dateparser->scrubadub>=2.0.0rc0->scrubadub_spacy) (0.1.0.post0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers<4.30.0,>=3.4.0->spacy-transformers<1.3.0,>=1.1.2->spacy[transformers]>=3.0.0->scrubadub_spacy) (2023.4.0)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.10/dist-packages (from pytz-deprecation-shim->tzlocal->dateparser->scrubadub>=2.0.0rc0->scrubadub_spacy) (2023.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->spacy-transformers<1.3.0,>=1.1.2->spacy[transformers]>=3.0.0->scrubadub_spacy) (1.3.0)\n",
            "Installing collected packages: scrubadub_spacy\n",
            "Successfully installed scrubadub_spacy-2.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install scrubadub_spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faP_QCEkCzzl",
        "outputId": "63ea5bea-93da-4fa1-a0bb-ec3edc6cbae5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy-transformers in /usr/local/lib/python3.10/dist-packages (1.2.4)\n",
            "Requirement already satisfied: spacy<4.0.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers) (3.5.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers) (1.22.4)\n",
            "Requirement already satisfied: transformers<4.30.0,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers) (4.29.2)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers) (2.4.6)\n",
            "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers) (0.9.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (1.1.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (4.65.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->spacy-transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->spacy-transformers) (16.0.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.30.0,>=3.4.0->spacy-transformers) (0.15.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.30.0,>=3.4.0->spacy-transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<4.30.0,>=3.4.0->spacy-transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.30.0,>=3.4.0->spacy-transformers) (0.13.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers<4.30.0,>=3.4.0->spacy-transformers) (2023.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.5.0->spacy-transformers) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.5.0->spacy-transformers) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4.0.0,>=3.5.0->spacy-transformers) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->spacy-transformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0NVTaftFu0o"
      },
      "source": [
        "**Remove Personal Identifiable Information**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Z914icc_FKP5",
        "outputId": "726e448f-e2f0-441b-ae47-a5ce18f0db61"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'My name is Alex, I work at LifeGuard in London, and my eMail is {{EMAIL}} btw. my super secret twitter login is username: {{USERNAME}} password: {{PASSWORD}}'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import scrubadub\n",
        "text = \"My name is Alex, I work at LifeGuard in London, and my eMail is alex@lifeguard.com btw. my super secret twitter login is username: alex_2000 password: g-dragon180888\"\n",
        "\n",
        "scrubadub.clean(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JdfPW2pHR-b",
        "outputId": "c9f93277-fbf8-4113-8a31-c72f541b7585"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scrubadub_stanford\n",
            "  Using cached scrubadub_stanford-2.1.3-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: nltk>=3.3 in /usr/local/lib/python3.10/dist-packages (from scrubadub_stanford) (3.8.1)\n",
            "Collecting stanza (from scrubadub_stanford)\n",
            "  Using cached stanza-1.5.0-py3-none-any.whl (802 kB)\n",
            "Requirement already satisfied: scrubadub>=2.0.0rc0 in /usr/local/lib/python3.10/dist-packages (from scrubadub_stanford) (2.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.3->scrubadub_stanford) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.3->scrubadub_stanford) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.3->scrubadub_stanford) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.3->scrubadub_stanford) (4.65.0)\n",
            "Requirement already satisfied: textblob==0.15.3 in /usr/local/lib/python3.10/dist-packages (from scrubadub>=2.0.0rc0->scrubadub_stanford) (0.15.3)\n",
            "Requirement already satisfied: phonenumbers in /usr/local/lib/python3.10/dist-packages (from scrubadub>=2.0.0rc0->scrubadub_stanford) (8.13.13)\n",
            "Requirement already satisfied: python-stdnum in /usr/local/lib/python3.10/dist-packages (from scrubadub>=2.0.0rc0->scrubadub_stanford) (1.18)\n",
            "Requirement already satisfied: dateparser in /usr/local/lib/python3.10/dist-packages (from scrubadub>=2.0.0rc0->scrubadub_stanford) (1.1.8)\n",
            "Requirement already satisfied: catalogue in /usr/local/lib/python3.10/dist-packages (from scrubadub>=2.0.0rc0->scrubadub_stanford) (2.0.8)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.10/dist-packages (from scrubadub>=2.0.0rc0->scrubadub_stanford) (0.0.post5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from scrubadub>=2.0.0rc0->scrubadub_stanford) (4.5.0)\n",
            "Requirement already satisfied: faker in /usr/local/lib/python3.10/dist-packages (from scrubadub>=2.0.0rc0->scrubadub_stanford) (18.10.1)\n",
            "Collecting emoji (from stanza->scrubadub_stanford)\n",
            "  Downloading emoji-2.4.0.tar.gz (353 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m353.7/353.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stanza->scrubadub_stanford) (1.22.4)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from stanza->scrubadub_stanford) (3.20.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from stanza->scrubadub_stanford) (2.27.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from stanza->scrubadub_stanford) (1.16.0)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from stanza->scrubadub_stanford) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza->scrubadub_stanford) (3.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza->scrubadub_stanford) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza->scrubadub_stanford) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza->scrubadub_stanford) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza->scrubadub_stanford) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.3.0->stanza->scrubadub_stanford) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.3.0->stanza->scrubadub_stanford) (16.0.5)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from dateparser->scrubadub>=2.0.0rc0->scrubadub_stanford) (2.8.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from dateparser->scrubadub>=2.0.0rc0->scrubadub_stanford) (2022.7.1)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from dateparser->scrubadub>=2.0.0rc0->scrubadub_stanford) (4.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->stanza->scrubadub_stanford) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->stanza->scrubadub_stanford) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->stanza->scrubadub_stanford) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->stanza->scrubadub_stanford) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->stanza->scrubadub_stanford) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.0->stanza->scrubadub_stanford) (1.3.0)\n",
            "Requirement already satisfied: pytz-deprecation-shim in /usr/local/lib/python3.10/dist-packages (from tzlocal->dateparser->scrubadub>=2.0.0rc0->scrubadub_stanford) (0.1.0.post0)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.10/dist-packages (from pytz-deprecation-shim->tzlocal->dateparser->scrubadub>=2.0.0rc0->scrubadub_stanford) (2023.3)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.4.0-py2.py3-none-any.whl size=350809 sha256=71d9b4c81e289701dc43c31d9c85204b96ea68fc0afbbaa983ad41529646a535\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/29/1c/234cae4632803c2ba4a76a71a679eb1383cf590775714e2a21\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji, stanza, scrubadub_stanford\n",
            "Successfully installed emoji-2.4.0 scrubadub_stanford-2.1.3 stanza-1.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install scrubadub_stanford"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2IBLS1rHrbi",
        "outputId": "887752bb-bb52-49c3-e26b-fea68c5c0210"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "laBH6YFCHLS4",
        "outputId": "8974776c-0e74-4a6a-8a4f-2c7f09f93802"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'My name is {{NAME}} and I work at the {{ORGANIZATION}} in {{LOCATION}}'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import scrubadub, scrubadub_stanford\n",
        "scrubber = scrubadub.Scrubber()\n",
        "scrubber.add_detector(scrubadub_stanford.detectors.StanfordEntityDetector(\n",
        "    enable_person=True, enable_organization=True, enable_location=True\n",
        "))\n",
        "scrubber.clean(\"My name is John and I work at the United Nations in Geneva\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vBQXUPBzYF5p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "98O-tZvJYF82"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.decodeschool.com/blog/Text-Mining/Centroid-based-Text-summarization-in-Python"
      ],
      "metadata": {
        "id": "F41jUVr3op83"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l-s4U_lhYJCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aUg0DJLlYJO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMMJzXo2HLWW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "ea2c9e78-6b81-4aa3-f5e8-267f7cda0bf7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f2c52bb1a2a8>\u001b[0m in \u001b[0;36m<cell line: 59>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m   \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"//content/drive/MyDrive/Hackathon/article\"\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mfile1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0mpage\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m   \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '//content/drive/MyDrive/Hackathon/articleCopy of roughness.pdf'"
          ]
        }
      ],
      "source": [
        "\n",
        "from os import listdir\n",
        "import string\n",
        "import math\n",
        "\n",
        "\"\"\"Method to calculate Inverse Document Frequency Score\"\"\"\n",
        "def calculate_idf(word):\n",
        "  files = [f for f in listdir(\"/content/drive/MyDrive/Hackathon/article\") ]  #Specify the directory where the documents located\n",
        "  count,wcount=2,1\n",
        "  for file1 in files:\n",
        "    file=open(\"/content/drive/MyDrive/Hackathon/article\" +file1,'r')     #Specify the directory where the documents located\n",
        "    page=file.read()\n",
        "    if(word in page):\n",
        "      wcount+=1\n",
        "      count+=1\n",
        "    idf=count/wcount\n",
        "\n",
        "    return math.log(idf,10)\n",
        "\n",
        "\"\"\"Method to calculate Centroid Score of sentences\"\"\"\n",
        "def calculate_centroid(sentences):\n",
        "\n",
        "    \"\"\"\"Compute tf X idf score for each word\"\"\"\n",
        "    tfidf=dict()\n",
        "    for sentence in sentences:\n",
        "        words=sentence.split()\n",
        "        for word in words:\n",
        "            if word in tfidf:\n",
        "                tfidf[word]+=calculate_idf(word)\n",
        "            else:\n",
        "                tfidf[word]=calculate_idf(word)\n",
        "\n",
        "    \"\"\"Construct the centroid of Cluster\n",
        "    By taking the words that are above the threshold\"\"\"\n",
        "\n",
        "    centroid=dict()\n",
        "    threshold=0.7\n",
        "    for word in tfidf:\n",
        "        if(tfidf[word]>threshold):\n",
        "            centroid[word]=tfidf[word]\n",
        "        else:\n",
        "            centroid[word]=0\n",
        "\n",
        "    \"\"\"Compute the Score for Sentences\"\"\"\n",
        "    senctence_score=list()\n",
        "    counter=0\n",
        "    for sentence in sentences:\n",
        "        senctence_score.append(0)\n",
        "        words=sentence.split()\n",
        "        for word in words:\n",
        "            senctence_score[counter]+=centroid[word]\n",
        "\n",
        "        counter=counter+1\n",
        "    return senctence_score\n",
        "\n",
        "\n",
        "\"\"\"Splitting Documents as sentences\"\"\"\n",
        "files = [f for f in listdir(\"/content/drive/MyDrive/Hackathon/article\") ]\n",
        "page=\"\"\n",
        "for file1 in files:\n",
        "  file = open(\"//content/drive/MyDrive/Hackathon/article\" +file1,'r')\n",
        "  page+=file.read()\n",
        "  file.close()\n",
        "\n",
        "\n",
        "sentences=page.split(\".\")\n",
        "senctence_score=calculate_centroid(sentences)\n",
        "\n",
        "\n",
        "\"\"\"Printing Sentences which has more central words\"\"\"\n",
        "for i in range(len(sentences)):\n",
        "  if(senctence_score[i]>15):\n",
        "    print(sentences[i])\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi-document Summarization**"
      ],
      "metadata": {
        "id": "7oEPYvlF1ca_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HEOcY-ZYbYr",
        "outputId": "6f9696fc-8504-4498-e1d3-1fca1938c883"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.2)\n",
            "Building wheels for collected packages: python-docx\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184491 sha256=c8d07bfd5c82a7d1645879b3f28025f6d64d1c54a44c7c7bb9d721a78bcef56c\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/27/06/837436d4c3bd989b957a91679966f207bfd71d358d63a8194d\n",
            "Successfully built python-docx\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-0.8.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NgiJXiTYbmD",
        "outputId": "90681785-6390-48f0-fd43-a72e96e6e37b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZJwc0zHYb0K",
        "outputId": "a55fb199-10bf-4951-a4e8-3ac235a71a19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "qSNASC1daG2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import docx\n",
        "import PyPDF2\n"
      ],
      "metadata": {
        "id": "JKQAYPqdd9KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Tokenize the text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and perform stemming\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stemmer = PorterStemmer()\n",
        "    preprocessed_sentences = []\n",
        "    for sentence in sentences:\n",
        "        words = nltk.word_tokenize(sentence)\n",
        "        filtered_words = [stemmer.stem(word.lower()) for word in words if word.lower() not in stop_words]\n",
        "        preprocessed_sentences.append(' '.join(filtered_words))\n",
        "\n",
        "    return preprocessed_sentences\n",
        "\n",
        "def read_text_from_docx(file_path):\n",
        "    doc = docx.Document('/content/drive/MyDrive/Hackathon/article/Copy of Project Methodology.docx')\n",
        "    text1 = ' '.join([paragraph.text for paragraph in doc.paragraphs])\n",
        "    return text1\n",
        "#text1 = read_text_from_docx('Copy of Project Methodology.docx')\n",
        "\n",
        "def read_text_from_pdf(file_path):\n",
        "    with open('/content/drive/MyDrive/Hackathon/article/Copy of roughness.pdf', 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        num_pages = len(reader.pages)\n",
        "        text2 = ''\n",
        "        for page in range(num_pages):\n",
        "            page_obj = reader.pages[page]\n",
        "            text2 += page_obj.extract_text()\n",
        "    return text2\n",
        "\n",
        "\n",
        "#text2 = read_text_from_pdf('Copy of roughness.pdf')\n",
        "\n",
        "def read_text_from_file(file_path):\n",
        "   _, ext = os.path.splitext(file_path)\n",
        "   if ext == '.docx':\n",
        "    return read_text_from_docx(file_path)\n",
        "   elif ext == '.pdf':\n",
        "    return read_text_from_pdf(file_path)\n",
        "   else:\n",
        "    with open(file_path, 'r') as f:\n",
        "       return f.read()"
      ],
      "metadata": {
        "id": "FJnwISVSdz7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_sentence_scores(sentences):\n",
        "    # Calculate the frequency of each word in the sentences\n",
        "    word_frequencies = {}\n",
        "    for sentence in sentences:\n",
        "        for word in nltk.word_tokenize(sentence):\n",
        "            if word not in word_frequencies:\n",
        "                word_frequencies[word] = 1\n",
        "            else:\n",
        "                word_frequencies[word] += 1\n",
        "\n",
        "    # Normalize the word frequencies\n",
        "    maximum_frequency = max(word_frequencies.values())\n",
        "    for word in word_frequencies.keys():\n",
        "        word_frequencies[word] = word_frequencies[word] / maximum_frequency\n",
        "\n",
        "    # Calculate the score of each sentence based on word frequencies\n",
        "    sentence_scores = {}\n",
        "    for sentence in sentences:\n",
        "        for word in nltk.word_tokenize(sentence):\n",
        "            if sentence not in sentence_scores:\n",
        "                sentence_scores[sentence] = word_frequencies[word]\n",
        "            else:\n",
        "                sentence_scores[sentence] += word_frequencies[word]\n",
        "\n",
        "    return sentence_scores\n",
        "\n",
        "def generate_summary(documents):\n",
        "    # Preprocess the text in the documents\n",
        "    preprocessed_documents = [preprocess_text(document) for document in documents]\n",
        "\n",
        "    # Combine all the preprocessed sentences into a single list\n",
        "    sentences = [sentence for document in preprocessed_documents for sentence in document]\n",
        "\n",
        "    # Calculate the scores of the sentences\n",
        "    sentence_scores = calculate_sentence_scores(sentences)\n",
        "\n",
        "    # Sort the sentences based on their scores in descending order\n",
        "    sorted_sentences = sorted(sentence_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Select the top n sentences as the summary\n",
        "    summary_sentences = [sentence for sentence, score in sorted_sentences[:3]]  # Change 3 to the desired number of sentences\n",
        "\n",
        "    # Combine the summary sentences into a single string\n",
        "    summary = ' '.join(summary_sentences)\n",
        "\n",
        "    return summary\n"
      ],
      "metadata": {
        "id": "rnf0XrlCeei3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iX71gYqaklvq",
        "outputId": "6ee32dfc-7f33-4347-c1f2-ee7102543824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUi00LLpks7N",
        "outputId": "5d069794-8933-490c-fb96-cff0df7964ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wAu05l9pjvS",
        "outputId": "f4eb0f24-265d-41d0-e7a3-3e47e2276e04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.2)\n",
            "Building wheels for collected packages: python-docx\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184491 sha256=37b611766412efe2eb6864bef54108d3d70956e985bd379a9931d84e75e7a9f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/27/06/837436d4c3bd989b957a91679966f207bfd71d358d63a8194d\n",
            "Successfully built python-docx\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-0.8.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = read_text_from_docx('Copy of Project Methodology.docx')\n",
        "text2 = read_text_from_pdf('Copy of roughness.pdf')\n",
        "# Example usage\n",
        "document_files = ['text1.docx', 'text2.pdf']\n",
        "\n",
        "documents = []\n",
        "for file in document_files:\n",
        "  document = read_text_from_file(file)\n",
        "  documents.append(document)\n",
        "\n",
        "summary = generate_summary(documents)\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "A5p0T1R7YJ78",
        "outputId": "613d6014-7afe-41e3-c692-ce8502a4d6fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-b015f359baf8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_text_from_docx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Copy of Project Methodology.docx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtext2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_text_from_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Copy of roughness.pdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdocument_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'text1.docx'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text2.pdf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-c91aa2ae5814>\u001b[0m in \u001b[0;36mread_text_from_docx\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_text_from_docx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Hackathon/article/Copy of Project Methodology.docx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mtext1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparagraph\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparagraphs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'docx' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k84z8h004eOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7fABT_mo4eQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eD0OfjvI4eT_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}